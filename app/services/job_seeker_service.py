from sqlalchemy.orm import Session, joinedload
from fastapi import UploadFile, HTTPException, status
from app.models.job_seeker import JobSeeker
from app.models.big5_test_result import Big5TestResult
from app.models.ai_learning_answer import AILearningAnswer
from app.models.ai_learning_question import AILearningQuestion
from app.models.job_seeker_document import JobSeekerDocument
from app.services.s3_service import S3Service
import uuid
import logging
import pandas as pd

logger = logging.getLogger(__name__)

class JobSeekerService:
    def __init__(self, db: Session):
        self.db = db
        self.s3_service = S3Service()
    
    def _to_uuid(self, user_id: str | uuid.UUID) -> uuid.UUID:
        if isinstance(user_id, uuid.UUID):
            return user_id
        return uuid.UUID(str(user_id))

    def _get_by_user_id(self, user_id: str | uuid.UUID) -> JobSeeker | None:
        uid = self._to_uuid(user_id)
        return self.db.query(JobSeeker).filter(JobSeeker.user_id == uid).first()

    def get_applicant_profile(self, user_id: str | uuid.UUID) -> JobSeeker | None:
        """ì§€ì›ì ë§ˆì´í˜ì´ì§€ ì •ë³´ ì¡°íšŒ: JobSeeker ORM ê°ì²´ ë°˜í™˜"""
        return self._get_by_user_id(user_id)
    
    def create_bio(self, user_id: str | uuid.UUID, bio: str) -> JobSeeker:
        """ì§€ì›ì ì§§ì€ì†Œê°œ ë“±ë¡"""
        applicant = self._get_by_user_id(user_id)
        if not applicant:
            applicant = JobSeeker(user_id=self._to_uuid(user_id), bio=bio)
            self.db.add(applicant)
        else:
            applicant.bio = bio
        
        self.db.commit()
        self.db.refresh(applicant)
        return applicant
    
    def update_bio(self, user_id: str | uuid.UUID, bio: str) -> JobSeeker:
        """ì§€ì›ì ì§§ì€ì†Œê°œ ìˆ˜ì •"""
        return self.create_bio(user_id, bio)
    
    def create_applicant_info(self, user_id: str | uuid.UUID, info_data: dict) -> JobSeeker:
        """ì§€ì›ì ê¸°ë³¸ì •ë³´ ë“±ë¡"""
        applicant = self._get_by_user_id(user_id)
        if not applicant:
            applicant = JobSeeker(user_id=self._to_uuid(user_id), **info_data)
            self.db.add(applicant)
        else:
            for key, value in info_data.items():
                setattr(applicant, key, value)
        
        self.db.commit()
        self.db.refresh(applicant)
        return applicant
    
    def update_applicant_info(self, user_id: str | uuid.UUID, info_data: dict) -> JobSeeker:
        """ì§€ì›ì ê¸°ë³¸ì •ë³´ ìˆ˜ì •"""
        return self.create_applicant_info(user_id, info_data)
    
    def get_mypage_data(self, user_id: str | uuid.UUID) -> dict:
        """ë§ˆì´í˜ì´ì§€ìš© ì¢…í•© ë°ì´í„° ì¡°íšŒ"""
        uid = self._to_uuid(user_id)
        
        # 1. JobSeeker ê¸°ë³¸ ì •ë³´ ì¡°íšŒ
        job_seeker = self.db.query(JobSeeker).filter(JobSeeker.user_id == uid).first()
        
        if not job_seeker:
            return None
        
        # 2. Big5 ì„±ê²©ê²€ì‚¬ ê²°ê³¼ ì¡°íšŒ - ìµœì‹  1ê±´ë§Œ
        big5_latest = self.db.query(Big5TestResult).filter(
            Big5TestResult.job_seeker_id == job_seeker.id
        ).order_by(Big5TestResult.test_date.desc()).first()
        
        # 3. AI í•™ìŠµ ì‘ë‹µ ì¡°íšŒ (ì§ˆë¬¸ ì •ë³´ í¬í•¨)
        ai_learning_answers = self.db.query(AILearningAnswer).options(
            joinedload(AILearningAnswer.question)
        ).filter(
            AILearningAnswer.job_seeker_id == job_seeker.id
        ).all()
        
        # 4. ë¬¸ì„œ ì¡°íšŒ
        documents = self.db.query(JobSeekerDocument).filter(
            JobSeekerDocument.job_seeker_id == job_seeker.id
        ).all()
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬ì„±
        return {
            'job_seeker': job_seeker,
            'big5_test_results': [big5_latest] if big5_latest else [],
            'ai_learning_answers': ai_learning_answers,
            'documents': documents,
            'behavior_text': getattr(job_seeker, 'behavior_text', None)
        }
    
    async def upload_file(self, user_id: str | uuid.UUID, document_type: str, file: UploadFile) -> dict:
        """S3 íŒŒì¼ ì—…ë¡œë“œ ë° DB ì €ì¥"""
        try:
            # 1. JobSeeker ì¡´ì¬ í™•ì¸
            uid = self._to_uuid(user_id)
            job_seeker = self.db.query(JobSeeker).filter(JobSeeker.user_id == uid).first()
            
            if not job_seeker:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="êµ¬ì§ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            
            # 2. S3ì— íŒŒì¼ ì—…ë¡œë“œ
            upload_result = await self.s3_service.upload_file(
                file=file,
                user_id=str(user_id),
                document_type=document_type
            )
            
            if not upload_result["success"]:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=upload_result["error"]
                )
            
            # 3. ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì •ë³´ ì €ì¥
            document = JobSeekerDocument(
                job_seeker_id=job_seeker.id,
                document_type=document_type,
                file_name=upload_result["original_filename"],
                file_url=upload_result["file_url"],
                file_size=upload_result["file_size"],
                mime_type=upload_result["content_type"]
            )
            
            self.db.add(document)
            self.db.commit()
            self.db.refresh(document)
            
            # 4. GitHub CSV íŒŒì¼ì¸ ê²½ìš° íŒŒì‹±í•˜ì—¬ github_repositories ì—…ë°ì´íŠ¸
            github_data = None
            if document_type == "github" and upload_result["content_type"] in ["text/csv", "application/csv"]:
                github_data = await self._parse_github_csv(upload_result["file_path"])
                if github_data:
                    job_seeker.github_repositories = github_data
                    self.db.add(job_seeker)
                    self.db.commit()
                    self.db.refresh(job_seeker)
            
            return {
                "success": True,
                "message": "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤",
                "document_id": str(document.id),
                "file_url": upload_result["file_url"],
                "file_name": upload_result["original_filename"],
                "file_size": upload_result["file_size"],
                "document_type": document_type,
                "user_id": str(user_id),
                "github_data": github_data
            }
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
    
    async def delete_file(self, user_id: str | uuid.UUID, document_type: str, file_name: str) -> dict:
        """íŒŒì¼ ì‚­ì œ ë° DBì—ì„œ ë ˆì½”ë“œ ì œê±°"""
        try:
            uid = self._to_uuid(user_id)
            
            # 1. JobSeeker ì¡´ì¬ í™•ì¸
            job_seeker = self.db.query(JobSeeker).filter(JobSeeker.user_id == uid).first()
            if not job_seeker:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="êµ¬ì§ì ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            
            # 2. DBì—ì„œ ë¬¸ì„œ ë ˆì½”ë“œ ì°¾ê¸°
            document = self.db.query(JobSeekerDocument).filter(
                JobSeekerDocument.job_seeker_id == job_seeker.id,
                JobSeekerDocument.document_type == document_type,
                JobSeekerDocument.file_name == file_name
            ).first()
            
            if not document:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
                )
            
            # 3. ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (UUID íŒŒì¼ëª… ì‚¬ìš©)
            # DBì— ì €ì¥ëœ file_urlì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¶”ì¶œ
            actual_file_path = None
            logger.info(f"ğŸ” ì‚­ì œí•  ë¬¸ì„œ ì •ë³´ - file_url: {document.file_url}, file_name: {document.file_name}")
            
            if document.file_url:
                # file_url: http://localhost:8000/files/{user_id}/{document_type}/{uuid_filename}
                # ì‹¤ì œ íŒŒì¼ ê²½ë¡œ: uploads/{user_id}/{document_type}/{uuid_filename}
                url_parts = document.file_url.split('/')
                logger.info(f"ğŸ” URL ë¶„í•  ê²°ê³¼: {url_parts}")
                if len(url_parts) >= 4:
                    uuid_filename = url_parts[-1]  # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ì‹¤ì œ íŒŒì¼ëª…
                    actual_file_path = f"uploads/{user_id}/{document_type}/{uuid_filename}"
                    logger.info(f"ğŸ” ì‹¤ì œ íŒŒì¼ ê²½ë¡œ: {actual_file_path}")
                else:
                    logger.warning(f"âš ï¸ URL í˜•ì‹ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„: {document.file_url}")
            else:
                logger.warning(f"âš ï¸ file_urlì´ ì—†ìŒ: {document.file_name}")
            
            # 4. ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ íŒŒì¼ ì‚­ì œ
            delete_result = {"success": True, "message": "íŒŒì¼ ì‚­ì œ ê±´ë„ˆëœ€"}
            if actual_file_path:
                logger.info(f"ğŸ—‘ï¸ íŒŒì¼ ì‚­ì œ ì‹œë„: {actual_file_path}")
                delete_result = await self.s3_service.delete_file(actual_file_path)
                if not delete_result["success"]:
                    # íŒŒì¼ì´ ì—†ì–´ë„ DB ë ˆì½”ë“œëŠ” ì‚­ì œ (ì •ë¦¬ ëª©ì )
                    logger.warning(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨í•˜ì§€ë§Œ DB ë ˆì½”ë“œëŠ” ì‚­ì œ: {delete_result['error']}")
                else:
                    logger.info(f"âœ… íŒŒì¼ ì‚­ì œ ì„±ê³µ: {actual_file_path}")
            else:
                logger.warning(f"ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ. DB ë ˆì½”ë“œë§Œ ì‚­ì œ: {document.file_url}")
            
            # 5. DBì—ì„œ ë ˆì½”ë“œ ì‚­ì œ
            self.db.delete(document)
            self.db.commit()
            
            return {
                "success": True,
                "message": "íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤",
                "deleted_file": {
                    "file_name": file_name,
                    "document_type": document_type,
                    "user_id": str(user_id)
                }
            }
            
        except HTTPException:
            raise
        except Exception as e:
            self.db.rollback()
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
    
    async def _parse_github_csv(self, file_path: str) -> dict:
        """GitHub CSV íŒŒì¼ì„ pandasë¡œ íŒŒì‹±í•˜ì—¬ usernameê³¼ repository URL ì¶”ì¶œ"""
        try:
            usernames = set()
            repositories = set()
            
            # pandasë¡œ CSV íŒŒì¼ ì½ê¸° (ì¸ì½”ë”© ìë™ ê°ì§€)
            try:
                # ì—¬ëŸ¬ ì¸ì½”ë”© ì‹œë„
                encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1']
                df = None
                
                for encoding in encodings:
                    try:
                        df = pd.read_csv(file_path, encoding=encoding, header=None)
                        logger.info(f"ğŸ“Š CSV íŒŒì¼ ì¸ì½”ë”© ê°ì§€: {encoding}")
                        break
                    except UnicodeDecodeError:
                        continue
                    except Exception as e:
                        logger.warning(f"âš ï¸ {encoding} ì¸ì½”ë”© ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
                        continue
                
                if df is None:
                    logger.error("âŒ ì§€ì›ë˜ëŠ” ì¸ì½”ë”©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    return None
                
                logger.info(f"ğŸ“Š CSV íŒŒì¼ ì½ê¸° ì„±ê³µ - í–‰ ìˆ˜: {len(df)}, ì—´ ìˆ˜: {len(df.columns)}")
                
                # ìµœì†Œ 2ê°œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
                if len(df.columns) < 2:
                    logger.error(f"âŒ CSV íŒŒì¼ì— ìµœì†Œ 2ê°œ ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤. í˜„ì¬: {len(df.columns)}ê°œ")
                    return None
                
                # ê° í–‰ ì²˜ë¦¬
                for index, row in df.iterrows():
                    if pd.notna(row.iloc[0]) and pd.notna(row.iloc[1]):
                        username = str(row.iloc[0]).strip()
                        repo_url = str(row.iloc[1]).strip()
                        
                        # usernameì´ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì¶”ê°€
                        if username and username != 'nan':
                            usernames.add(username)
                        
                        # repository URLì´ ë¹„ì–´ìˆì§€ ì•Šê³  GitHub URLì¸ì§€ í™•ì¸
                        if repo_url and repo_url != 'nan' and ('github.com' in repo_url or repo_url.startswith('http')):
                            repositories.add(repo_url)
                    else:
                        logger.warning(f"âš ï¸ {index+1}í–‰: ë¹ˆ ê°’ì´ ìˆìŠµë‹ˆë‹¤")
                
                logger.info(f"ğŸ“Š GitHub CSV íŒŒì‹± ì™„ë£Œ - ì‚¬ìš©ì: {len(usernames)}ê°œ, ì €ì¥ì†Œ: {len(repositories)}ê°œ")
                logger.info(f"ğŸ“Š ì‚¬ìš©ì ëª©ë¡: {list(usernames)}")
                logger.info(f"ğŸ“Š ì €ì¥ì†Œ ëª©ë¡: {list(repositories)}")
                
                return {
                    "username": list(usernames),
                    "repository": list(repositories)
                }
                
            except Exception as e:
                logger.error(f"âŒ pandas CSV ì½ê¸° ì‹¤íŒ¨: {e}")
                return None
            
        except Exception as e:
            logger.error(f"âŒ GitHub CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
